# Readme for N(t) example
This directory has a quick example of a workflow for estimating changing 
population size over time from a simulated sample ($N(t)$) using
stairwayplot. The basic workflow is organized using `snakemake`
so the only step needed to perform the simulation and run 
the estimation is

`$ snakemake -j 8`

In this case the `-j 8` option tells `snakemake` to use 8 cores for 
the workflow. If more cores are available by all means increase this 
number.

If something goes wrong or you need to start from scratch I've
included a `clean` rule in the `Snakefile`, so you can clean 
everything out using

`$ snakemake clean`

## Workflow


The analysis includes three programs for predicting effective population 
size through time(`n_t`): 
[msmc](https://github.com/stschiff/msmc/issues/23) 
[stairwayplot](https://sites.google.com/site/jpopgen/stairway-plot), and
[smc++](https://github.com/popgenmethods/smcpp).
There are four target rules that can be run with the given parameters: 
`compound_msmc`,
`compound_smcpp`,
`compound_stairwayplot`, or you can run all three on the same simulations with
`all`.

As of now, you can run any combination of `Knobs` found in `Snakefile`.
These parameters can be adjusted
for an analysis run by simply setting the value 
of the `python` variables at the top of the file. These variables include:

`seed` : `<class 'int'>` 
This sets the seed such that any anaysis configuration
and run can be replicated exactly. 

`num_sampled_genomes_per_replicate` : `<class 'int'>` 
This is the haploid number
of genomes to simulate. All samples will be fed into the input files for both
`smc++` and `stairwayplot`.

`num_sampled_genomes_msmc` : `<class 'list[int]'>` 
Because we are interested in the 
variance of predictions from `msmc` with differing number of samples,
you can specify an array of sample sizes to run. For each of these sample sizes,
`replicates` number analyses will be run with `msmc`. All of these numbers should
not exceed `num_sampled_genomes_per_replicate` because the input files are 
generated by pruning the tree sequences from the original simulations.
We might want to do this for the other two programs as well

`replicates` : `<class 'int'>` The number of replicate simulations to run and 
analyze. 

`num_msmc_iterations` : `<class 'int'>` The number of iterations for each `msmc` 
run

`output_dir` : `<class 'str'>` This where all the intermediate files and results
will be stored. This includes all plots preduced as well as all input/output files
from the simulations and software runs organized into subdirectories named 
replicate seed.

`species` : `<class 'module'>` from `stdpopsim` such as `stdpopsim.homo_sapiens`.
Chromosomes, demographics, and recombination maps should be derived from this.

`model` : `<class childclass Model>` 
This is the class that defines the demography associated with a species. All models
should inherit from `<class Model>`

`genetic_map` : `<class childclass GeneticMap>` This will define the genetic map
used for simulations.

`chrm_list` : `<class 'list[str]'>` The chromosomes you would like to simulate
for input into each of the analysis run. All chromosomes simulated will be fed
as a single input into each analysis by the inference programs, for each replicate.

### Dependencies
Most dependencies can be loaded in using `pip install -r requirements.txt`. 
An exception
to this is `smc++` which uses `conda` for its install. The simplest install
should be achieved using

 `$ conda install -c terhorst -c bioconda smcpp`

Further details on `smc++` and its use can be found 
[here](https://github.com/popgenmethods/smcpp)

### Cluster environments
Our workflow can also be run on a cluster. To do so requires
the setup of a `.json` configuration file that lets `snakemake`
know about your cluster. We have provided an example of 
such a file in `cluster_talapas.json` that is for use with a
University of Oregon SLURM cluster. At a minimum, you should
edit the names of the partition to match those on your own HPC.
The workflow can then be launched with the call

`$ snakemake -j 999 --cluster-config cluster_talapas.json --cluster "sbatch -p {cluster.partition} -n {cluster.n}  -t {cluster.time}"`

and jobs will be automatically farmed out to the cluster

### Final output
The current final output is a plot comparing stairwayplot and smc++ estimates of $N(t)$, i.e., `homo_sapiens_Gutenkunst/all_estimated_Ne.png`  
